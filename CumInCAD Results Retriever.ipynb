{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "url = \"http://papers.cumincad.org/cgi-bin/works/Search?search=Scanner&x=0&y=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, fileName):\n",
    "    \"\"\"Run the other functions to turn a CumInCAD search into a json of published paper data\"\"\"\n",
    "    \n",
    "    # Important initialisations\n",
    "    nextURL = url\n",
    "    listings = []\n",
    "    pageCount = 0\n",
    "    \n",
    "    # Loop through each page of search results starting at the specified url, gathering links to listings\n",
    "    while nextURL != \"lastPage\":\n",
    "        # Get the HTML\n",
    "        html = getHTML(nextURL)\n",
    "        # Split the HTML into listings and add to the list of listings\n",
    "        listings.extend(getListings(html))\n",
    "        # Find the link to the next page\n",
    "        nextURL = getNextURL(html)\n",
    "        # Count and display the amount of pages\n",
    "        pageCount += 1\n",
    "        print(\"Page number \" + str(pageCount))\n",
    "    \n",
    "    # Extract just the URL of each listing\n",
    "    listings = [getListingURL(listing) for listing in listings]\n",
    "    \n",
    "    # Looping through each of the listing pages to mine data\n",
    "    listingCount = 0\n",
    "    listingsData = []\n",
    "    for listing in listings[:4]:\n",
    "        # Counting the listings\n",
    "        listingCount += 1\n",
    "        print(\"\\n--------\\n\\n\\nListing \" + str(listingCount))\n",
    "        \n",
    "        # Getting the HTML\n",
    "        listingHTML = getHTML(listing)\n",
    "        \n",
    "        # Scraping the metadata from the html\n",
    "        dataPoints = scrapeListingMeta(listingHTML)\n",
    "        \n",
    "        # Adding the data to the large collection of it\n",
    "        listingsData.append(dataPoints)\n",
    "    \n",
    "    # Save the data as a json string\n",
    "    jsonDataString = json.dumps(listingsData)\n",
    "    \n",
    "    # Save the string to a file\n",
    "    with open(fileName + \".json\", 'w') as jsonFile:\n",
    "        jsonFile.write(jsonDataString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextURL(html):\n",
    "    \"\"\"Find the next page's URL from within the previous's HTML\"\"\"\n",
    "    \n",
    "    # Split the html at the point where the next page icon is used\n",
    "    parts = html.split('=\"/woda/icons/flat-noborder/forward.gif\"')\n",
    "    \n",
    "    # If this icon doesn't occur, return the 'lastPage' flag\n",
    "    if len(parts) == 1:\n",
    "        return \"lastPage\"\n",
    "    else:\n",
    "        # Getting the URL\n",
    "        nextURL = \"http://papers.cumincad.org\" + parts[0].split(\"HREF\")[-1].split('\"')[1]\n",
    "        \n",
    "        # Replacing &amp; with &\n",
    "        nextURL = nextURL.replace(\"&amp;\", \"&\")\n",
    "        \n",
    "        # Returning the new URL\n",
    "        return nextURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTML(url):\n",
    "    \"\"\"Get the HTML at a given URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListings(fullHTML):\n",
    "    \"\"\"Get the listings from the HTML\"\"\"\n",
    "    \n",
    "    # Cut down to the relevant <div> \n",
    "    div = fullHTML.split(\"<DIV CLASS=RECORDS>\")[1].split(\"</DIV>\")[0]\n",
    "    \n",
    "    # Get all the table rows, excluding the bit of non-table stuff at the beginning and the first header row\n",
    "    table = [(\"<tr\" + row).strip() for row in div.split(\"<tr\")[2:]]\n",
    "    \n",
    "    # Get rid of a little extraneous bit at the end of the last row\n",
    "    if table[-1].endswith(\"</tbody></table><div><br/></div>\"):\n",
    "        table[-1] = table[-1][:-32].strip()\n",
    "    \n",
    "    # Joining rows that shouldn't be seperated (there's one row inside each row)\n",
    "    rowPairs = list(zip(table[::2], table[1::2]))\n",
    "    rows = [pair[0] + pair[1] for pair in rowPairs]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListingURL(listing):\n",
    "    \"\"\"Get the URL that corresponds to the detailed page of a listing\"\"\"\n",
    "    return listing.split(\"<A HREF=\")[1].split(\">\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeListingMeta(html):\n",
    "    \"\"\"Return a dictionary of all the metadata in the html of a listing's page\"\"\"\n",
    "    \n",
    "    # Retrieving a list of each meta point in the HTML\n",
    "    metaPoints = html.split(\"<meta\")[1:]\n",
    "    metaPoints = [point.split(\"/>\")[0] for point in metaPoints]\n",
    "    \n",
    "    # Looping through the points, adding the data from each string to a dictionary\n",
    "    listingMeta = {}\n",
    "    badListingMeta = {}\n",
    "    for point in metaPoints:\n",
    "        # Split the point up into valuable information\n",
    "        point = point.strip()\n",
    "        pointParts = point.split('\"')\n",
    "        \n",
    "        # Add the parts appropriately to the dictionary\n",
    "        listingMeta[pointParts[1]] = pointParts[3]\n",
    "        \n",
    "        # Note down the potentially bad data in a different dictionary\n",
    "        if point.count('\"') != 4:\n",
    "            badListingMeta[pointParts[1]] = pointParts[3]\n",
    "    \n",
    "    \n",
    "    #print(\"\\n\\nlistingMeta:\")\n",
    "    #[print(meta + \": \" + listingMeta[meta]) for meta in listingMeta]\n",
    "    #print(\"\\nbadListingMeta:\")\n",
    "    #[print(meta + \": \" + badListingMeta[meta]) for meta in badListingMeta]\n",
    "    \n",
    "    return listingMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page number 1\n",
      "Page number 2\n",
      "\n",
      "--------\n",
      "\n",
      "\n",
      "Listing 1\n",
      "\n",
      "--------\n",
      "\n",
      "\n",
      "Listing 2\n",
      "\n",
      "--------\n",
      "\n",
      "\n",
      "Listing 3\n",
      "\n",
      "--------\n",
      "\n",
      "\n",
      "Listing 4\n"
     ]
    }
   ],
   "source": [
    "main(url, \"testData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
